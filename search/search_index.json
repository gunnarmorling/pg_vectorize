{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pg_vectorize: a VectorDB for Postgres A Postgres extension that automates the transformation and orchestration of text to embeddings and provides hooks into the most popular LLMs. This allows you to do vector search and build LLM applications on existing data with as little as two function calls. This project relies heavily on the work by pgvector for vector similarity search and pgmq for orchestration in background workers. API Documentation : https://tembo-io.github.io/pg_vectorize/ Source : https://github.com/tembo-io/pg_vectorize Features \u00b6 Workflows for both vector search and RAG Integrations with OpenAI's embeddings and chat-completion endpoints and a self-hosted container for running Hugging Face Sentence-Transformers Automated creation of Postgres triggers to keep your embeddings up to date High level API - one function to initialize embeddings transformations, and another function to search Table of Contents \u00b6 Features Table of Contents Installation Vector Search Example RAG Example Trigger based updates Try it on Tembo Cloud Installation \u00b6 The fastest way to get started is by running the Tembo docker container and the vector server with docker compose: docker compose up -d Then connect to Postgres: docker compose exec -it postgres psql Enable the extension and its dependencies CREATE EXTENSION vectorize CASCADE ; Install into an existing Postgres instance If you're installing in an existing Postgres instance, you will need the following dependencies: Rust: - [pgrx toolchain](https://github.com/pgcentralfoundation/pgrx) Postgres Extensions: - [pg_cron](https://github.com/citusdata/pg_cron) ^1.5 - [pgmq](https://github.com/tembo-io/pgmq) ^1 - [pgvector](https://github.com/pgvector/pgvector) ^0.5.0 Then set the following either in postgresql.conf or as a configuration parameter: -- requires restart of Postgres alter system set shared_preload_libraries = 'vectorize,pg_cron' ; alter system set cron . database_name = 'postgres' And if you're running the vector-serve container, set the following url as a configuration parameter in Postgres. The host may need to change from `localhost` to something else depending on where you are running the container. alter system set vectorize . embedding_service_url = 'http://localhost:3000/v1/embeddings' SELECT pg_reload_conf (); Vector Search Example \u00b6 Text-to-embedding transformation can be done with either Hugging Face's Sentence-Transformers or OpenAI's embeddings. The following examples use Hugging Face's Sentence-Transformers. See the project documentation for OpenAI examples. Follow the installation steps if you haven't already. Setup a products table. Copy from the example data provided by the extension. CREATE TABLE products AS SELECT * FROM vectorize . example_products ; SELECT * FROM products limit 2 ; product_id | product_name | description | last_updated_at ------------+--------------+--------------------------------------------------------+------------------------------- 1 | Pencil | Utensil used for writing and often works best on paper | 2023-07-26 17:20:43.639351-05 2 | Laptop Stand | Elevated platform for laptops, enhancing ergonomics | 2023-07-26 17:20:43.639351-05 Create a job to vectorize the products table. We'll specify the tables primary key (product_id) and the columns that we want to search (product_name and description). SELECT vectorize . table ( job_name => 'product_search_hf' , \"table\" => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1' ); This adds a new column to your table, in our case it is named product_search_embeddings , then populates that data with the transformed embeddings from the product_name and description columns. Then search, SELECT * FROM vectorize . search ( job_name => 'product_search_hf' , query => 'accessories for mobile devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 ); search_results --------------------------------------------------------------------------------------------- { \"product_id\" : 13 , \"product_name\" : \"Phone Charger\" , \"similarity_score\" : 0 . 8147814132322894 } { \"product_id\" : 6 , \"product_name\" : \"Backpack\" , \"similarity_score\" : 0 . 7743061352550308 } { \"product_id\" : 11 , \"product_name\" : \"Stylus Pen\" , \"similarity_score\" : 0 . 7709902653575383 } RAG Example \u00b6 Ask raw text questions of the example products dataset and get chat responses from an OpenAI LLM. Follow the installation steps if you haven't already. Set the OpenAI API key , this is required to for use with OpenAI's chat-completion models. ALTER SYSTEM SET vectorize . openai_key TO '<your api key>' ; SELECT pg_reload_conf (); Create an example table if it does not already exist. CREATE TABLE products AS SELECT * FROM vectorize . example_products ; Initialize a table for RAG. We'll use an open source Sentence Transformer to generate embeddings. Create a new column that we want to use as the context. In this case, we'll concatenate both product_name and description . ALTER TABLE products ADD COLUMN context TEXT GENERATED ALWAYS AS ( product_name || ': ' || description ) STORED ; SELECT vectorize . init_rag ( agent_name => 'product_chat' , table_name => 'products' , \"column\" => 'context' , unique_record_id => 'product_id' , transformer => 'sentence-transformers/all-MiniLM-L12-v2' ); SELECT vectorize . rag ( agent_name => 'product_chat' , query => 'What is a pencil?' ) -> 'chat_response' ; \"A pencil is an item that is commonly used for writing and is known to be most effective on paper.\" Trigger based updates \u00b6 When vectorize job is set up as realtime (the default behavior, via vectorize.table(..., schedule => 'realtime') ), vectorize will create triggers on your table that will keep your embeddings up to date. When the text inputs are updated or if new rows are inserted, the triggers handle creating a background job that updates the embeddings. Since the transformation is executed in a background job and the transformer model is invoked in a separate container, there is minimal impact on the performance of the update or insert statement. INSERT INTO products ( product_id , product_name , description ) VALUES ( 12345 , 'pizza' , 'dish of Italian origin consisting of a flattened disk of bread' ); UPDATE products SET description = 'sling made of fabric, rope, or netting, suspended between two or more points, used for swinging, sleeping, or resting' WHERE product_name = 'Hammock' ; Try it on Tembo Cloud \u00b6 Try it for yourself! Install with a single click on a Vector DB Stack (or any other instance) in Tembo Cloud today.","title":"Vectorize"},{"location":"#features","text":"Workflows for both vector search and RAG Integrations with OpenAI's embeddings and chat-completion endpoints and a self-hosted container for running Hugging Face Sentence-Transformers Automated creation of Postgres triggers to keep your embeddings up to date High level API - one function to initialize embeddings transformations, and another function to search","title":"Features"},{"location":"#table-of-contents","text":"Features Table of Contents Installation Vector Search Example RAG Example Trigger based updates Try it on Tembo Cloud","title":"Table of Contents"},{"location":"#installation","text":"The fastest way to get started is by running the Tembo docker container and the vector server with docker compose: docker compose up -d Then connect to Postgres: docker compose exec -it postgres psql Enable the extension and its dependencies CREATE EXTENSION vectorize CASCADE ; Install into an existing Postgres instance If you're installing in an existing Postgres instance, you will need the following dependencies: Rust: - [pgrx toolchain](https://github.com/pgcentralfoundation/pgrx) Postgres Extensions: - [pg_cron](https://github.com/citusdata/pg_cron) ^1.5 - [pgmq](https://github.com/tembo-io/pgmq) ^1 - [pgvector](https://github.com/pgvector/pgvector) ^0.5.0 Then set the following either in postgresql.conf or as a configuration parameter: -- requires restart of Postgres alter system set shared_preload_libraries = 'vectorize,pg_cron' ; alter system set cron . database_name = 'postgres' And if you're running the vector-serve container, set the following url as a configuration parameter in Postgres. The host may need to change from `localhost` to something else depending on where you are running the container. alter system set vectorize . embedding_service_url = 'http://localhost:3000/v1/embeddings' SELECT pg_reload_conf ();","title":"Installation"},{"location":"#vector-search-example","text":"Text-to-embedding transformation can be done with either Hugging Face's Sentence-Transformers or OpenAI's embeddings. The following examples use Hugging Face's Sentence-Transformers. See the project documentation for OpenAI examples. Follow the installation steps if you haven't already. Setup a products table. Copy from the example data provided by the extension. CREATE TABLE products AS SELECT * FROM vectorize . example_products ; SELECT * FROM products limit 2 ; product_id | product_name | description | last_updated_at ------------+--------------+--------------------------------------------------------+------------------------------- 1 | Pencil | Utensil used for writing and often works best on paper | 2023-07-26 17:20:43.639351-05 2 | Laptop Stand | Elevated platform for laptops, enhancing ergonomics | 2023-07-26 17:20:43.639351-05 Create a job to vectorize the products table. We'll specify the tables primary key (product_id) and the columns that we want to search (product_name and description). SELECT vectorize . table ( job_name => 'product_search_hf' , \"table\" => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1' ); This adds a new column to your table, in our case it is named product_search_embeddings , then populates that data with the transformed embeddings from the product_name and description columns. Then search, SELECT * FROM vectorize . search ( job_name => 'product_search_hf' , query => 'accessories for mobile devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 ); search_results --------------------------------------------------------------------------------------------- { \"product_id\" : 13 , \"product_name\" : \"Phone Charger\" , \"similarity_score\" : 0 . 8147814132322894 } { \"product_id\" : 6 , \"product_name\" : \"Backpack\" , \"similarity_score\" : 0 . 7743061352550308 } { \"product_id\" : 11 , \"product_name\" : \"Stylus Pen\" , \"similarity_score\" : 0 . 7709902653575383 }","title":"Vector Search Example"},{"location":"#rag-example","text":"Ask raw text questions of the example products dataset and get chat responses from an OpenAI LLM. Follow the installation steps if you haven't already. Set the OpenAI API key , this is required to for use with OpenAI's chat-completion models. ALTER SYSTEM SET vectorize . openai_key TO '<your api key>' ; SELECT pg_reload_conf (); Create an example table if it does not already exist. CREATE TABLE products AS SELECT * FROM vectorize . example_products ; Initialize a table for RAG. We'll use an open source Sentence Transformer to generate embeddings. Create a new column that we want to use as the context. In this case, we'll concatenate both product_name and description . ALTER TABLE products ADD COLUMN context TEXT GENERATED ALWAYS AS ( product_name || ': ' || description ) STORED ; SELECT vectorize . init_rag ( agent_name => 'product_chat' , table_name => 'products' , \"column\" => 'context' , unique_record_id => 'product_id' , transformer => 'sentence-transformers/all-MiniLM-L12-v2' ); SELECT vectorize . rag ( agent_name => 'product_chat' , query => 'What is a pencil?' ) -> 'chat_response' ; \"A pencil is an item that is commonly used for writing and is known to be most effective on paper.\"","title":"RAG Example"},{"location":"#trigger-based-updates","text":"When vectorize job is set up as realtime (the default behavior, via vectorize.table(..., schedule => 'realtime') ), vectorize will create triggers on your table that will keep your embeddings up to date. When the text inputs are updated or if new rows are inserted, the triggers handle creating a background job that updates the embeddings. Since the transformation is executed in a background job and the transformer model is invoked in a separate container, there is minimal impact on the performance of the update or insert statement. INSERT INTO products ( product_id , product_name , description ) VALUES ( 12345 , 'pizza' , 'dish of Italian origin consisting of a flattened disk of bread' ); UPDATE products SET description = 'sling made of fabric, rope, or netting, suspended between two or more points, used for swinging, sleeping, or resting' WHERE product_name = 'Hammock' ;","title":"Trigger based updates"},{"location":"#try-it-on-tembo-cloud","text":"Try it for yourself! Install with a single click on a Vector DB Stack (or any other instance) in Tembo Cloud today.","title":"Try it on Tembo Cloud"},{"location":"api/","text":"PG Vectorize API Overview \u00b6 pg vectorize provides tools for two closely related tasks; vector search and retrieval augmented generation (RAG), and there are APIs dedicated to both of these tasks. Vector search is an important component of RAG and the RAG APIs depend on the vector search APIs. It could be helpful to think of the vector search APIs as lower level than RAG. However, relative to Postgres's APIs, both of these vectorize APIs are very high level.","title":"Overview"},{"location":"api/#pg-vectorize-api-overview","text":"pg vectorize provides tools for two closely related tasks; vector search and retrieval augmented generation (RAG), and there are APIs dedicated to both of these tasks. Vector search is an important component of RAG and the RAG APIs depend on the vector search APIs. It could be helpful to think of the vector search APIs as lower level than RAG. However, relative to Postgres's APIs, both of these vectorize APIs are very high level.","title":"PG Vectorize API Overview"},{"location":"api/rag/","text":"RAG \u00b6 SQL API for Retrieval Augmented Generation projects. Initializing a RAG table \u00b6 Creates embeddings for specified data in a Postgres table. Creates index, and triggers to keep embeddings up to date. vectorize.init_rag \u00b6 vectorize . init_rag ( \"agent_name\" TEXT , \"table_name\" TEXT , \"unique_record_id\" TEXT , \"column\" TEXT , \"schema\" TEXT DEFAULT 'public' , \"transformer\" TEXT DEFAULT 'text-embedding-ada-002' , \"search_alg\" vectorize . SimilarityAlg DEFAULT 'pgv_cosine_similarity' , \"table_method\" vectorize . TableMethod DEFAULT 'append' ) RETURNS TEXT Parameters: Parameter Type Description agent_name text A unique name for the project. table_name text The name of the table to be initialized. unique_record_id text The name of the column that contains the unique record id. column text The name of the column that contains the content that is used for context for RAG. schema text The name of the schema where the table is located. Defaults to 'public'. transformer text The name of the transformer to use for the embeddings. Defaults to 'text-embedding-ada-002'. search_alg SimilarityAlg The name of the search algorithm to use. Defaults to 'pgv_cosine_similarity'. table_method TableMethod The method to use for the table. Defaults to 'append', which adds a column to the existing table. Example: select vectorize . init_rag ( agent_name => 'tembo_chat' , table_name => 'tembo_docs' , unique_record_id => 'document_name' , \"column\" => 'content' , transformer => 'sentence-transformers/all-MiniLM-L12-v2' ); Query using RAG \u00b6 vectorize.rag \u00b6 vectorize . \"rag\" ( \"agent_name\" TEXT , \"query\" TEXT , \"chat_model\" TEXT DEFAULT 'gpt-3.5-turbo' , \"task\" TEXT DEFAULT 'question_answer' , \"api_key\" TEXT DEFAULT NULL , \"num_context\" INT DEFAULT 2 , \"force_trim\" bool DEFAULT false ) RETURNS TABLE ( \"chat_results\" jsonb ) Parameters: Parameter Type Description agent_name text Specify the name provided during vectorize.init_rag query text The user provided query or command provided to the chat completion model. task text Specifies the name of the prompt template to use. Must exist in vectorize.prompts (prompt_type) api_key text API key for the specified chat model. If OpenAI, this value overrides the config vectorize.openai_key num_context int The number of context documents returned by similarity search include in the message submitted to the chat completion model force_trim bool Trims the documents provided as context, starting with the least relevant documents, such that the prompt fits into the model's context window. Defaults to false. Example \u00b6 select vectorize . rag ( agent_name => 'tembo_support' , query => 'what are the major features from the tembo kubernetes operator?' , chat_model => 'gpt-3.5-turbo' , force_trim => 'true' ); The response contains the contextual data used in the prompt in addition to the chat response. { \"context\" : [ { \"content\" : \"\\\"Tembo Standard Stack\\\\n\\\\nThe Tembo Standard Stack is a tuned Postgres instance balance for general purpose computing. You have full control over compute, configuration, and extension installation.\\\"\" , \"token_ct\" : 37 , \"record_id\" : \"535\" }, { \"content\" : \"\\\"Why Stacks?\\\\n\\\\nAdopting a new database adds significant complexity and costs to an engineering organization. Organizations spend a huge amount of time evaluating, benchmarking or migrating databases and setting upcomplicated pipelines keeping those databases in sync.\\\\n\\\\nMost of these use cases can be served by Postgres, thanks to its stability, feature completeness and extensibility. However, optimizing Postgres for each use case is a non-trivial task and requires domain expertise, use case understanding and deep Postgres expertise, making it hard for most developers to adopt this.\\\\n\\\\nTembo Stacks solve that problem by providing pre-built, use case optimized Postgres deployments.\\\\n\\\\nA tembo stack is a pre-built, use case specific Postgres deployment which enables you to quickly deploy specialized data services that can replace external, non-Postgres data services. They help you avoid the pains associated with adopting, operationalizing, optimizing and managing new databases.\\\\n\\\\n|Name|Replacement for|\\\\n|----|---------------|\\\\n|Data Warehouse| Snowflake, Bigquery |\\\\n|Geospatial| ESRI, Oracle |\\\\n|OLTP| Amazon RDS |\\\\n|OLAP| Snowflake, Bigquery |\\\\n|Machine Learning| MindsDB |\\\\n|Message Queue| Amazon SQS, RabbitMQ, Redis |\\\\n|Mongo Alternative on Postgres| MongoDB |\\\\n|RAG| LangChain |\\\\n|Standard| Amazon RDS |\\\\n|Vector DB| Pinecone, Weaviate |\\\\n\\\\nWe are actively working on additional Stacks. Check out the Tembo Roadmap and upvote the stacks you''d like to see next.\\\"\" , \"token_ct\" : 336 , \"record_id\" : \"387\" } ], \"chat_response\" : \"Tembo Stacks are pre-built, use case specific Postgres deployments that are optimized for various data services such as Data Warehouse, Geospatial, OLTP, OLAP, Machine Learning, Message Queue, and more. These Stacks aim to provide organizations with specialized data services that can replace external non-Postgres data services. Each Tembo Stack is designed to cater to specific use cases, enabling developers to quickly deploy and utilize Postgres instances tailored to their needs without the complexity of setting up and optimizing Postgres manually.\" } Filter the results to just the chat_response : select vectorize . rag ( agent_name => 'tembo_support' , query => 'what are the major features from the tembo kubernetes operator?' , chat_model => 'gpt-3.5-turbo' , force_trim => 'true' ) -> 'chat_response' ; \"Tembo Stacks are pre-built, use case specific Postgres deployments that are optimized for various data services such as Data Warehouse, Geospatial, OLTP, OLAP, Machine Learning, Message Queue, and more. These Stacks aim to provide organizations with specialized data services that can replace external non-Postgres data services. Each Tembo Stack is designed to cater to specific use cases, enabling developers to quickly deploy and utilize Postgres instances tailored to their needs without the complexity of setting up and optimizing Postgres manually.\"","title":"RAG"},{"location":"api/rag/#rag","text":"SQL API for Retrieval Augmented Generation projects.","title":"RAG"},{"location":"api/rag/#initializing-a-rag-table","text":"Creates embeddings for specified data in a Postgres table. Creates index, and triggers to keep embeddings up to date.","title":"Initializing a RAG table"},{"location":"api/rag/#vectorizeinit_rag","text":"vectorize . init_rag ( \"agent_name\" TEXT , \"table_name\" TEXT , \"unique_record_id\" TEXT , \"column\" TEXT , \"schema\" TEXT DEFAULT 'public' , \"transformer\" TEXT DEFAULT 'text-embedding-ada-002' , \"search_alg\" vectorize . SimilarityAlg DEFAULT 'pgv_cosine_similarity' , \"table_method\" vectorize . TableMethod DEFAULT 'append' ) RETURNS TEXT Parameters: Parameter Type Description agent_name text A unique name for the project. table_name text The name of the table to be initialized. unique_record_id text The name of the column that contains the unique record id. column text The name of the column that contains the content that is used for context for RAG. schema text The name of the schema where the table is located. Defaults to 'public'. transformer text The name of the transformer to use for the embeddings. Defaults to 'text-embedding-ada-002'. search_alg SimilarityAlg The name of the search algorithm to use. Defaults to 'pgv_cosine_similarity'. table_method TableMethod The method to use for the table. Defaults to 'append', which adds a column to the existing table. Example: select vectorize . init_rag ( agent_name => 'tembo_chat' , table_name => 'tembo_docs' , unique_record_id => 'document_name' , \"column\" => 'content' , transformer => 'sentence-transformers/all-MiniLM-L12-v2' );","title":"vectorize.init_rag"},{"location":"api/rag/#query-using-rag","text":"","title":"Query using RAG"},{"location":"api/rag/#vectorizerag","text":"vectorize . \"rag\" ( \"agent_name\" TEXT , \"query\" TEXT , \"chat_model\" TEXT DEFAULT 'gpt-3.5-turbo' , \"task\" TEXT DEFAULT 'question_answer' , \"api_key\" TEXT DEFAULT NULL , \"num_context\" INT DEFAULT 2 , \"force_trim\" bool DEFAULT false ) RETURNS TABLE ( \"chat_results\" jsonb ) Parameters: Parameter Type Description agent_name text Specify the name provided during vectorize.init_rag query text The user provided query or command provided to the chat completion model. task text Specifies the name of the prompt template to use. Must exist in vectorize.prompts (prompt_type) api_key text API key for the specified chat model. If OpenAI, this value overrides the config vectorize.openai_key num_context int The number of context documents returned by similarity search include in the message submitted to the chat completion model force_trim bool Trims the documents provided as context, starting with the least relevant documents, such that the prompt fits into the model's context window. Defaults to false.","title":"vectorize.rag"},{"location":"api/rag/#example","text":"select vectorize . rag ( agent_name => 'tembo_support' , query => 'what are the major features from the tembo kubernetes operator?' , chat_model => 'gpt-3.5-turbo' , force_trim => 'true' ); The response contains the contextual data used in the prompt in addition to the chat response. { \"context\" : [ { \"content\" : \"\\\"Tembo Standard Stack\\\\n\\\\nThe Tembo Standard Stack is a tuned Postgres instance balance for general purpose computing. You have full control over compute, configuration, and extension installation.\\\"\" , \"token_ct\" : 37 , \"record_id\" : \"535\" }, { \"content\" : \"\\\"Why Stacks?\\\\n\\\\nAdopting a new database adds significant complexity and costs to an engineering organization. Organizations spend a huge amount of time evaluating, benchmarking or migrating databases and setting upcomplicated pipelines keeping those databases in sync.\\\\n\\\\nMost of these use cases can be served by Postgres, thanks to its stability, feature completeness and extensibility. However, optimizing Postgres for each use case is a non-trivial task and requires domain expertise, use case understanding and deep Postgres expertise, making it hard for most developers to adopt this.\\\\n\\\\nTembo Stacks solve that problem by providing pre-built, use case optimized Postgres deployments.\\\\n\\\\nA tembo stack is a pre-built, use case specific Postgres deployment which enables you to quickly deploy specialized data services that can replace external, non-Postgres data services. They help you avoid the pains associated with adopting, operationalizing, optimizing and managing new databases.\\\\n\\\\n|Name|Replacement for|\\\\n|----|---------------|\\\\n|Data Warehouse| Snowflake, Bigquery |\\\\n|Geospatial| ESRI, Oracle |\\\\n|OLTP| Amazon RDS |\\\\n|OLAP| Snowflake, Bigquery |\\\\n|Machine Learning| MindsDB |\\\\n|Message Queue| Amazon SQS, RabbitMQ, Redis |\\\\n|Mongo Alternative on Postgres| MongoDB |\\\\n|RAG| LangChain |\\\\n|Standard| Amazon RDS |\\\\n|Vector DB| Pinecone, Weaviate |\\\\n\\\\nWe are actively working on additional Stacks. Check out the Tembo Roadmap and upvote the stacks you''d like to see next.\\\"\" , \"token_ct\" : 336 , \"record_id\" : \"387\" } ], \"chat_response\" : \"Tembo Stacks are pre-built, use case specific Postgres deployments that are optimized for various data services such as Data Warehouse, Geospatial, OLTP, OLAP, Machine Learning, Message Queue, and more. These Stacks aim to provide organizations with specialized data services that can replace external non-Postgres data services. Each Tembo Stack is designed to cater to specific use cases, enabling developers to quickly deploy and utilize Postgres instances tailored to their needs without the complexity of setting up and optimizing Postgres manually.\" } Filter the results to just the chat_response : select vectorize . rag ( agent_name => 'tembo_support' , query => 'what are the major features from the tembo kubernetes operator?' , chat_model => 'gpt-3.5-turbo' , force_trim => 'true' ) -> 'chat_response' ; \"Tembo Stacks are pre-built, use case specific Postgres deployments that are optimized for various data services such as Data Warehouse, Geospatial, OLTP, OLAP, Machine Learning, Message Queue, and more. These Stacks aim to provide organizations with specialized data services that can replace external non-Postgres data services. Each Tembo Stack is designed to cater to specific use cases, enabling developers to quickly deploy and utilize Postgres instances tailored to their needs without the complexity of setting up and optimizing Postgres manually.\"","title":"Example"},{"location":"api/search/","text":"Vector Search \u00b6 The vector-search flow is two part; first initialize a table using vectorize.table() , then search the table with vectorize.search() . Initialize a table \u00b6 Initialize a table for vector search. Generates embeddings and index. Creates triggers to keep embeddings up-to-date. vectorize . \"table\" ( \"table\" TEXT , \"columns\" TEXT [], \"job_name\" TEXT , \"primary_key\" TEXT , \"args\" json DEFAULT '{}' , \"schema\" TEXT DEFAULT 'public' , \"update_col\" TEXT DEFAULT 'last_updated_at' , \"transformer\" TEXT DEFAULT 'text-embedding-ada-002' , \"search_alg\" vectorize . SimilarityAlg DEFAULT 'pgv_cosine_similarity' , \"table_method\" vectorize . TableMethod DEFAULT 'append' , \"schedule\" TEXT DEFAULT 'realtime' ) RETURNS TEXT Parameter Type Description table text The name of the table to be initialized. columns text The name of the columns that contains the content that is used for context for RAG. Multiple columns are concatenated. job_name text A unique name for the project. primary_key text The name of the column that contains the unique record id. args json Additional arguments for the transformer. Defaults to '{}'. schema text The name of the schema where the table is located. Defaults to 'public'. update_col text Column specifying the last time the record was updated. Required for cron-like schedule. Defaults to last_updated_at transformer text The name of the transformer to use for the embeddings. Defaults to 'text-embedding-ada-002'. search_alg SimilarityAlg The name of the search algorithm to use. Defaults to 'pgv_cosine_similarity'. table_method TableMethod The method to use for the table. Defaults to 'append', which adds a column to the existing table. schedule text 'realtime' by default for trigger based updates. accepts a cron-like input for a cron based updates. Sentence-Transformer Examples \u00b6 OpenAI Examples \u00b6 To use embedding model provided by OpenAI's public embedding endpoints, provide the model name into the transformer parameter, and provide the OpenAI API key. Pass the API key into the function call via args . select vectorize . table ( job_name => 'product_search' , \"table\" => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'text-embedding-ada-002' , args => '{\"api_key\": \"my-openai-key\"}' ); The API key can also be set via GUC. ALTER SYSTEM SET vectorize . openai_key TO 'my-openai-key' ; SELECT pg_reload_conf (); Then call vectorize.table() without providing the API key. select vectorize . table ( job_name => 'product_search' , \"table\" => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'text-embedding-ada-002' ); Search a table \u00b6 Search a table initialized with vectorize.table . The search results are sorted in descending order according to similarity. The query is transformed to embeddings using the same transformer configured during vectorize.table . vectorize . \"search\" ( \"job_name\" TEXT , \"query\" TEXT , \"api_key\" TEXT DEFAULT NULL , \"return_columns\" TEXT [] DEFAULT ARRAY [ '*' ]:: text [], \"num_results\" INT DEFAULT 10 ) RETURNS TABLE ( \"search_results\" jsonb ) Parameters: Parameter Type Description job_name text A unique name for the project. query text The user provided query or command provided to the chat completion model. api_key text API key for the specified chat model. If OpenAI, this value overrides the config vectorize.openai_key return_columns text[] The columns to return in the search results. Defaults to all columns. num_results int The number of results to return. Sorted in descending order according to similarity. Defaults to 10. Example \u00b6 SELECT * FROM vectorize . search ( job_name => 'product_search' , query => 'mobile electronic devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 ); search_results -------------------------------------------------------------------------------------------- ---- {\"product_id\": 13, \"product_name\": \"Phone Charger\", \"similarity_score\": 0.8564681325237845} {\"product_id\": 24, \"product_name\": \"Tablet Holder\", \"similarity_score\": 0.8295988934993099} {\"product_id\": 4, \"product_name\": \"Bluetooth Speaker\", \"similarity_score\": 0.8250355616233103} (3 rows)","title":"Vector Search"},{"location":"api/search/#vector-search","text":"The vector-search flow is two part; first initialize a table using vectorize.table() , then search the table with vectorize.search() .","title":"Vector Search"},{"location":"api/search/#initialize-a-table","text":"Initialize a table for vector search. Generates embeddings and index. Creates triggers to keep embeddings up-to-date. vectorize . \"table\" ( \"table\" TEXT , \"columns\" TEXT [], \"job_name\" TEXT , \"primary_key\" TEXT , \"args\" json DEFAULT '{}' , \"schema\" TEXT DEFAULT 'public' , \"update_col\" TEXT DEFAULT 'last_updated_at' , \"transformer\" TEXT DEFAULT 'text-embedding-ada-002' , \"search_alg\" vectorize . SimilarityAlg DEFAULT 'pgv_cosine_similarity' , \"table_method\" vectorize . TableMethod DEFAULT 'append' , \"schedule\" TEXT DEFAULT 'realtime' ) RETURNS TEXT Parameter Type Description table text The name of the table to be initialized. columns text The name of the columns that contains the content that is used for context for RAG. Multiple columns are concatenated. job_name text A unique name for the project. primary_key text The name of the column that contains the unique record id. args json Additional arguments for the transformer. Defaults to '{}'. schema text The name of the schema where the table is located. Defaults to 'public'. update_col text Column specifying the last time the record was updated. Required for cron-like schedule. Defaults to last_updated_at transformer text The name of the transformer to use for the embeddings. Defaults to 'text-embedding-ada-002'. search_alg SimilarityAlg The name of the search algorithm to use. Defaults to 'pgv_cosine_similarity'. table_method TableMethod The method to use for the table. Defaults to 'append', which adds a column to the existing table. schedule text 'realtime' by default for trigger based updates. accepts a cron-like input for a cron based updates.","title":"Initialize a table"},{"location":"api/search/#sentence-transformer-examples","text":"","title":"Sentence-Transformer Examples"},{"location":"api/search/#openai-examples","text":"To use embedding model provided by OpenAI's public embedding endpoints, provide the model name into the transformer parameter, and provide the OpenAI API key. Pass the API key into the function call via args . select vectorize . table ( job_name => 'product_search' , \"table\" => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'text-embedding-ada-002' , args => '{\"api_key\": \"my-openai-key\"}' ); The API key can also be set via GUC. ALTER SYSTEM SET vectorize . openai_key TO 'my-openai-key' ; SELECT pg_reload_conf (); Then call vectorize.table() without providing the API key. select vectorize . table ( job_name => 'product_search' , \"table\" => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'text-embedding-ada-002' );","title":"OpenAI Examples"},{"location":"api/search/#search-a-table","text":"Search a table initialized with vectorize.table . The search results are sorted in descending order according to similarity. The query is transformed to embeddings using the same transformer configured during vectorize.table . vectorize . \"search\" ( \"job_name\" TEXT , \"query\" TEXT , \"api_key\" TEXT DEFAULT NULL , \"return_columns\" TEXT [] DEFAULT ARRAY [ '*' ]:: text [], \"num_results\" INT DEFAULT 10 ) RETURNS TABLE ( \"search_results\" jsonb ) Parameters: Parameter Type Description job_name text A unique name for the project. query text The user provided query or command provided to the chat completion model. api_key text API key for the specified chat model. If OpenAI, this value overrides the config vectorize.openai_key return_columns text[] The columns to return in the search results. Defaults to all columns. num_results int The number of results to return. Sorted in descending order according to similarity. Defaults to 10.","title":"Search a table"},{"location":"api/search/#example","text":"SELECT * FROM vectorize . search ( job_name => 'product_search' , query => 'mobile electronic devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 ); search_results -------------------------------------------------------------------------------------------- ---- {\"product_id\": 13, \"product_name\": \"Phone Charger\", \"similarity_score\": 0.8564681325237845} {\"product_id\": 24, \"product_name\": \"Tablet Holder\", \"similarity_score\": 0.8295988934993099} {\"product_id\": 4, \"product_name\": \"Bluetooth Speaker\", \"similarity_score\": 0.8250355616233103} (3 rows)","title":"Example"},{"location":"api/utilities/","text":"Utilities \u00b6 Text to Embeddings \u00b6 Transforms a block of text to embeddings using the specified transformer. Requires the vector-serve container to be set via vectorize.embedding_svc_url , or an OpenAI key to be set if using OpenAI embedding models. vectorize . \"transform_embeddings\" ( \"input\" TEXT , \"model_name\" TEXT DEFAULT 'text-embedding-ada-002' , \"api_key\" TEXT DEFAULT NULL ) RETURNS double precision [] Parameters: Parameter Type Description input text Raw text to be transformed to an embedding model_name text Name of the sentence-transformer or OpenAI model to use. api_key text API key for the transformer. Defaults to NULL. Example \u00b6 select vectorize . transform_embeddings ( input => 'the quick brown fox jumped over the lazy dogs' , model_name => 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1' ); { - 0 . 2556323707103729 , - 0 . 3213586211204529 ..., - 0 . 0951206386089325 }","title":"Utilities"},{"location":"api/utilities/#utilities","text":"","title":"Utilities"},{"location":"api/utilities/#text-to-embeddings","text":"Transforms a block of text to embeddings using the specified transformer. Requires the vector-serve container to be set via vectorize.embedding_svc_url , or an OpenAI key to be set if using OpenAI embedding models. vectorize . \"transform_embeddings\" ( \"input\" TEXT , \"model_name\" TEXT DEFAULT 'text-embedding-ada-002' , \"api_key\" TEXT DEFAULT NULL ) RETURNS double precision [] Parameters: Parameter Type Description input text Raw text to be transformed to an embedding model_name text Name of the sentence-transformer or OpenAI model to use. api_key text API key for the transformer. Defaults to NULL.","title":"Text to Embeddings"},{"location":"api/utilities/#example","text":"select vectorize . transform_embeddings ( input => 'the quick brown fox jumped over the lazy dogs' , model_name => 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1' ); { - 0 . 2556323707103729 , - 0 . 3213586211204529 ..., - 0 . 0951206386089325 }","title":"Example"},{"location":"examples/openai_embeddings/","text":"Vector Search with OpenAI \u00b6 First you'll need an OpenAI API key . Set your API key as a Postgres configuration parameter. ALTER SYSTEM SET vectorize . openai_key TO '<your api key>' ; SELECT pg_reload_conf (); Create an example table if it does not already exist. CREATE TABLE products AS SELECT * FROM vectorize . example_products ; Then create the job. It may take some time to generate embeddings, depending on API latency. SELECT vectorize . table ( job_name => 'product_search_openai' , \"table\" => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'text-embedding-ada-002' ); To search the table, use the vectorize.search function. SELECT * FROM vectorize . search ( job_name => 'product_search_openai' , query => 'accessories for mobile devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 ); search_results -------------------------------------------------------------------------------------------- ---- {\"product_id\": 13, \"product_name\": \"Phone Charger\", \"similarity_score\": 0.8564681325237845} {\"product_id\": 24, \"product_name\": \"Tablet Holder\", \"similarity_score\": 0.8295988934993099} {\"product_id\": 4, \"product_name\": \"Bluetooth Speaker\", \"similarity_score\": 0.8250355616233103} (3 rows)","title":"Vector Search with OpenAI"},{"location":"examples/openai_embeddings/#vector-search-with-openai","text":"First you'll need an OpenAI API key . Set your API key as a Postgres configuration parameter. ALTER SYSTEM SET vectorize . openai_key TO '<your api key>' ; SELECT pg_reload_conf (); Create an example table if it does not already exist. CREATE TABLE products AS SELECT * FROM vectorize . example_products ; Then create the job. It may take some time to generate embeddings, depending on API latency. SELECT vectorize . table ( job_name => 'product_search_openai' , \"table\" => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'text-embedding-ada-002' ); To search the table, use the vectorize.search function. SELECT * FROM vectorize . search ( job_name => 'product_search_openai' , query => 'accessories for mobile devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 ); search_results -------------------------------------------------------------------------------------------- ---- {\"product_id\": 13, \"product_name\": \"Phone Charger\", \"similarity_score\": 0.8564681325237845} {\"product_id\": 24, \"product_name\": \"Tablet Holder\", \"similarity_score\": 0.8295988934993099} {\"product_id\": 4, \"product_name\": \"Bluetooth Speaker\", \"similarity_score\": 0.8250355616233103} (3 rows)","title":"Vector Search with OpenAI"},{"location":"examples/sentence_transformers/","text":"Sentence Transformers \u00b6 Setup a products table. Copy from the example data provided by the extension. Ensure vectorize.embedding_svc_url is set to the URL of the vector-serve container. If you're running this example using the docker-compose.yaml file from this repo, it should look like this: SHOW vectorize . embedding_service_url ; vectorize.embedding_service_url ---------------------------------------- http://vector-serve:3000/v1/embeddings (1 row) If you are not running in docker, then you will need to change the url to the appropriate location. If that is localhost, it would look like this; ALTER SYSTEM SET vectorize . embedding_svc_url TO 'http://localhost:3000/v1/embeddings' ; Then reload Postgres configurations: SELECT pg_reload_conf (); Create an example table if it does not already exist. CREATE TABLE products AS SELECT * FROM vectorize . example_products ; SELECT * FROM products limit 2 ; product_id | product_name | description | last_updated_at ------------+--------------+--------------------------------------------------------+------------------------------- 1 | Pencil | Utensil used for writing and often works best on paper | 2023-07-26 17:20:43.639351-05 2 | Laptop Stand | Elevated platform for laptops, enhancing ergonomics | 2023-07-26 17:20:43.639351-05 Create a job to vectorize the products table. We'll specify the tables primary key (product_id) and the columns that we want to search (product_name and description). SELECT vectorize . table ( job_name => 'product_search_hf' , \"table\" => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1' ); This adds a new column to your table, in our case it is named product_search_embeddings , then populates that data with the transformed embeddings from the product_name and description columns. Then search, SELECT * FROM vectorize . search ( job_name => 'product_search_hf' , query => 'accessories for mobile devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 ); search_results --------------------------------------------------------------------------------------------- { \"product_id\" : 13 , \"product_name\" : \"Phone Charger\" , \"similarity_score\" : 0 . 8147814132322894 } { \"product_id\" : 6 , \"product_name\" : \"Backpack\" , \"similarity_score\" : 0 . 7743061352550308 } { \"product_id\" : 11 , \"product_name\" : \"Stylus Pen\" , \"similarity_score\" : 0 . 7709902653575383 }","title":"Sentence Transformers"},{"location":"examples/sentence_transformers/#sentence-transformers","text":"Setup a products table. Copy from the example data provided by the extension. Ensure vectorize.embedding_svc_url is set to the URL of the vector-serve container. If you're running this example using the docker-compose.yaml file from this repo, it should look like this: SHOW vectorize . embedding_service_url ; vectorize.embedding_service_url ---------------------------------------- http://vector-serve:3000/v1/embeddings (1 row) If you are not running in docker, then you will need to change the url to the appropriate location. If that is localhost, it would look like this; ALTER SYSTEM SET vectorize . embedding_svc_url TO 'http://localhost:3000/v1/embeddings' ; Then reload Postgres configurations: SELECT pg_reload_conf (); Create an example table if it does not already exist. CREATE TABLE products AS SELECT * FROM vectorize . example_products ; SELECT * FROM products limit 2 ; product_id | product_name | description | last_updated_at ------------+--------------+--------------------------------------------------------+------------------------------- 1 | Pencil | Utensil used for writing and often works best on paper | 2023-07-26 17:20:43.639351-05 2 | Laptop Stand | Elevated platform for laptops, enhancing ergonomics | 2023-07-26 17:20:43.639351-05 Create a job to vectorize the products table. We'll specify the tables primary key (product_id) and the columns that we want to search (product_name and description). SELECT vectorize . table ( job_name => 'product_search_hf' , \"table\" => 'products' , primary_key => 'product_id' , columns => ARRAY [ 'product_name' , 'description' ], transformer => 'sentence-transformers/multi-qa-MiniLM-L6-dot-v1' ); This adds a new column to your table, in our case it is named product_search_embeddings , then populates that data with the transformed embeddings from the product_name and description columns. Then search, SELECT * FROM vectorize . search ( job_name => 'product_search_hf' , query => 'accessories for mobile devices' , return_columns => ARRAY [ 'product_id' , 'product_name' ], num_results => 3 ); search_results --------------------------------------------------------------------------------------------- { \"product_id\" : 13 , \"product_name\" : \"Phone Charger\" , \"similarity_score\" : 0 . 8147814132322894 } { \"product_id\" : 6 , \"product_name\" : \"Backpack\" , \"similarity_score\" : 0 . 7743061352550308 } { \"product_id\" : 11 , \"product_name\" : \"Stylus Pen\" , \"similarity_score\" : 0 . 7709902653575383 }","title":"Sentence Transformers"}]}